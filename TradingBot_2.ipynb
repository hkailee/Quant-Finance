{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "os.environ['TZ'] = 'Asia/Singapore'  # to set timezone; needed when running on cloud\n",
    "time.tzset()\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 300,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 60\n",
    "    }\n",
    "\n",
    "OUTPUT_PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time(text, stime):\n",
    "    seconds = (time.time()-stime)\n",
    "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1545169500000</th>\n",
       "      <td>164.95</td>\n",
       "      <td>165.4999</td>\n",
       "      <td>164.95</td>\n",
       "      <td>165.32</td>\n",
       "      <td>28400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545170400000</th>\n",
       "      <td>164.70</td>\n",
       "      <td>165.2200</td>\n",
       "      <td>164.70</td>\n",
       "      <td>164.81</td>\n",
       "      <td>15517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545171300000</th>\n",
       "      <td>164.99</td>\n",
       "      <td>164.9900</td>\n",
       "      <td>164.60</td>\n",
       "      <td>164.75</td>\n",
       "      <td>7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545172200000</th>\n",
       "      <td>164.90</td>\n",
       "      <td>165.0200</td>\n",
       "      <td>164.51</td>\n",
       "      <td>165.00</td>\n",
       "      <td>11887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545173100000</th>\n",
       "      <td>164.75</td>\n",
       "      <td>166.0700</td>\n",
       "      <td>164.70</td>\n",
       "      <td>164.75</td>\n",
       "      <td>3434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545174000000</th>\n",
       "      <td>164.89</td>\n",
       "      <td>164.9400</td>\n",
       "      <td>164.71</td>\n",
       "      <td>164.75</td>\n",
       "      <td>4026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545174900000</th>\n",
       "      <td>166.07</td>\n",
       "      <td>166.0700</td>\n",
       "      <td>164.95</td>\n",
       "      <td>164.95</td>\n",
       "      <td>17873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545175800000</th>\n",
       "      <td>165.20</td>\n",
       "      <td>165.3200</td>\n",
       "      <td>165.13</td>\n",
       "      <td>165.32</td>\n",
       "      <td>1664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545176700000</th>\n",
       "      <td>165.21</td>\n",
       "      <td>165.2100</td>\n",
       "      <td>165.18</td>\n",
       "      <td>165.18</td>\n",
       "      <td>1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545177600000</th>\n",
       "      <td>165.10</td>\n",
       "      <td>165.2300</td>\n",
       "      <td>165.00</td>\n",
       "      <td>165.23</td>\n",
       "      <td>8012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                close      high     low    open  volume\n",
       "datetime                                               \n",
       "1545169500000  164.95  165.4999  164.95  165.32   28400\n",
       "1545170400000  164.70  165.2200  164.70  164.81   15517\n",
       "1545171300000  164.99  164.9900  164.60  164.75    7635\n",
       "1545172200000  164.90  165.0200  164.51  165.00   11887\n",
       "1545173100000  164.75  166.0700  164.70  164.75    3434\n",
       "1545174000000  164.89  164.9400  164.71  164.75    4026\n",
       "1545174900000  166.07  166.0700  164.95  164.95   17873\n",
       "1545175800000  165.20  165.3200  165.13  165.32    1664\n",
       "1545176700000  165.21  165.2100  165.18  165.18    1845\n",
       "1545177600000  165.10  165.2300  165.00  165.23    8012"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('robolee/Data/AAPL.csv', index_col='datetime')\n",
    "data_cols = ['close', 'high', 'low', 'open', 'volume']\n",
    "df = df[data_cols]\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if any null values are present\n",
      " close     0\n",
      "high      0\n",
      "low       0\n",
      "open      0\n",
      "volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"checking if any null values are present\\n\", df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test size 139691 34923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_cols = [\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train and Test size\", len(df_train), len(df_test))\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/leehongkai/anaconda/envs/trading_py36/lib/python3.6/site-packages (4.32.2)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    TIME_STEPS = 3\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    \n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(LSTM(60, dropout=0.0))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a01b51189da4613b85bee07e9e641d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=139688), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (139688, 3, 5) (139688,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8a694ec57a4d568293fe28b52550d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34920), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (34920, 3, 5) (34920,)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found\n",
      "Building model...\n",
      "checking if GPU available []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have shape (60, 5) but got array with shape (3, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e6d75d175efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n\u001b[1;32m     29\u001b[0m                         shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n\u001b[0;32m---> 30\u001b[0;31m                         trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/trading_py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/trading_py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/trading_py36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have shape (60, 5) but got array with shape (3, 5)"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "try:\n",
    "    model = pickle.load(open(\"lstm_model\", 'rb'))\n",
    "    print(\"Loaded saved model...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")\n",
    "    \n",
    "is_update_model = True\n",
    "if model is None or is_update_model:\n",
    "    from keras import backend as K\n",
    "    print(\"Building model...\")\n",
    "    print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "    model = create_model()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "                          \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    # Not used here. But leaving it here as a reminder for future\n",
    "    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "    history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    pickle.dump(model, open(\"lstm_model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
